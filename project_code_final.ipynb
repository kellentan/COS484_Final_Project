{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68cd05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kc4642/.conda/envs/torch-nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% Import Statements (may be more than necessary)\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BartTokenizer, DataCollatorForSeq2Seq, BartForConditionalGeneration\n",
    "from transformers import AdamW, Seq2SeqTrainer, Seq2SeqTrainingArguments, get_scheduler, Trainer, TrainingArguments, GenerationConfig, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fe4f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA A100-SXM4-80GB\n",
      "Thu May  2 20:00:41 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\r\n",
      "| N/A   32C    P0             61W /  500W |       4MiB /  81920MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# %% Ensure CUDA Availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fc8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Random Seed Function (Charles)\n",
    "from transformers.utils import logging\n",
    "\n",
    "def set_logging_and_seed(seed=42):\n",
    "    # Set logging output settings\n",
    "    logging.set_verbosity_info()\n",
    "    logger = logging.get_logger(\"transformers\")\n",
    "    logging.set_verbosity(30)\n",
    "    logger.warning(\"WARN\")\n",
    "\n",
    "    # Set random seed outputs\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if (torch.cuda.is_available()): torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da76a552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Load ATOMIC Dataset (Charles)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "atomic_train = pd.read_csv(\"/scratch/gpfs/kc4642/Datasets/ATOMIC/train.tsv\", sep=\"\\t\").values.tolist()\n",
    "atomic_train = [s for s in atomic_train if \"_\" not in s[0] and type(s[-1]) == type(\"\") and len(s[-1].split(\" \")) == 1 and s[-1] != \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180c79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['inputs', 'labels'],\n",
      "    num_rows: 3809\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# %% Construct ATOMIC Dataset (Charles)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "relationship_dictionary = {\n",
    "    \"xWant\" : \". As a result, PersonX wants \",\n",
    "    \"oEffect\" : \". As a result, Y or others will \", \n",
    "    \"oReact\" : \". As a result, Y or others feels \",\n",
    "    \"xReason\" : \" because \",\n",
    "    \"xNeed\" : \". But before, PersonX needed \",\n",
    "    \"AtLocation\" : \" located or found at \",\n",
    "    \"xEffect\" : \". As a result, PersonX will \",\n",
    "    \"ObjectUse\" : \" used for \",\n",
    "    \"MadeUpOf\" : \" made up of \",\n",
    "    \"CapableOf\" : \" is capable of \",\n",
    "    \"Causes\" : \" causes \",\n",
    "    \"HinderedBy\" : \" can be hindered by \",\n",
    "    \"xIntent\" : \" because PersonX wanted \",\n",
    "    \"xReact\" : \". As a result, PersonX feels \",\n",
    "    \"Desires\" : \" desires \",\n",
    "    \"HasProperty\" : \" can be characterized as having \",\n",
    "    \"NotDesires\" : \" does not desire \",\n",
    "    \"oWant\" : \". As a result, Y or others want \",\n",
    "    \"xAttr\" : \". PersonX is seen as \",\n",
    "    \"HasSubEvent\" : \" includes the event \"}\n",
    "\n",
    "atomic_training_dataset = {\"inputs\" : [], \"labels\" : []}\n",
    "for i in range(len(atomic_train)):\n",
    "    label = atomic_train[i][-1]\n",
    "    sentence = atomic_train[i][0] + relationship_dictionary[atomic_train[i][1]] + \"<mask>.\"\n",
    "    \n",
    "    if (atomic_train[i][1] not in [\"MadeUpOf\", \"HasProperty\"]): continue\n",
    "    atomic_training_dataset[\"inputs\"].append(sentence)\n",
    "    atomic_training_dataset[\"labels\"].append(label)\n",
    "    \n",
    "atomic_training_dataset = Dataset.from_dict(atomic_training_dataset).shuffle(seed=42)\n",
    "print(atomic_training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9077d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Load NumerSense Train Dataset (Mahsa)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe999183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['inputs', 'labels'],\n",
      "    num_rows: 10443\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# %% Create NumerSense Train Dataset (Mahsa)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "training_dataset = {\"inputs\" : [], \"labels\" : []}\n",
    "for i in range(len(train)):\n",
    "    training_dataset[\"inputs\"].append(train[i][0])\n",
    "    training_dataset[\"labels\"].append(train[i][1]) # label\n",
    "    pass\n",
    "\n",
    "training_dataset = Dataset.from_dict(training_dataset).shuffle(seed=42)\n",
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c803e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Define model & tokenizer (Kellen)\n",
    "from transformers import AutoModel, AutoModelForMaskedLM, AutoModelForSeq2SeqLM # AutoModelWithLMHead\n",
    "from transformers import AutoModelForCausalLM, AutoModelWithLMHead, AutoTokenizer\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "model_path = \"/scratch/gpfs/kc4642/Models/bart-large\"\n",
    "tokenizer_path = \"/scratch/gpfs/kc4642/Tokenizers/bart-large-tokenizer\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Add pad token if not previously seen\n",
    "if tokenizer.mask_token != \"<mask>\":\n",
    "    tokenizer.add_special_tokens({\"mask_token\" : \"<mask>\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "# Freeze the base model if required\n",
    "freeze = False\n",
    "\n",
    "if (freeze == True):\n",
    "    for param in model.roberta.parameters(): param.requires_grad = False # model.bert.parameters() for BERT-Large\n",
    "    # for param in model.lm_head.parameters(): param.requires_grad = True # Uncomment for BART-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88743d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "# %% Tokenize Dataset (Charles)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "max_input_length = max_target_length = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples[\"inputs\"]\n",
    "    labels = examples[\"labels\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "    model_labels = tokenizer(labels, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
    "    \n",
    "    padded_labels = []\n",
    "    for label_example in model_labels:\n",
    "        temp_label = [label if label != tokenizer.pad_token_id else -100 for label in label_example]\n",
    "        padded_labels.append(temp_label)\n",
    "    \n",
    "    model_inputs[\"labels\"] = padded_labels\n",
    "    # model_inputs[\"labels\"] = model_labels\n",
    "    return model_inputs\n",
    "\n",
    "# Map our tokenization scheme onto our datasets (uncomment whatever is necessary)\n",
    "encoded_train = training_dataset.map(tokenize_function, batched=True)\n",
    "# encoded_train = atomic_training_dataset.map(tokenize_function, batched=True)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Manual Training Data Preparation (Kellen)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "# Remove unnecessary column names from our data\n",
    "encoded_train = encoded_train.remove_columns([\"inputs\"])\n",
    "encoded_train.set_format(\"torch\")\n",
    "\n",
    "# Create training dataloader\n",
    "train_dataloader = DataLoader(encoded_train, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Curriculum Learning (Kellen)\n",
    "from transformers import get_scheduler, AdamW\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Define lr scheduler\n",
    "num_epochs = 15\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\"linear\",\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=num_training_steps)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps), position=0, leave=True)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Store our curriculum scores\n",
    "    softmax_scores = []\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Append our per sample softmax scores\n",
    "        logits = outputs.logits\n",
    "        sample_index, mask_token_index = (batch[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "        # softmax_logits = torch.softmax(logits[sample_index, mask_token_index, :], dim=1)\n",
    "        # scores = softmax_logits[sample_index, batch[\"labels\"][:, 1]]\n",
    "        \n",
    "        scores = logits[sample_index, mask_token_index, :][sample_index, batch[\"labels\"][:, 1]]\n",
    "        softmax_scores += scores.tolist()\n",
    "        epoch_loss += loss.item()\n",
    "        pass\n",
    "    \n",
    "    # Update our curriculum\n",
    "    all_scores = np.array(softmax_scores)\n",
    "    subset_idx = np.argsort(all_scores)[::-1] # sort scores in descending order, maximal scores first\n",
    "    encoded_train = encoded_train.select(subset_idx)\n",
    "    train_dataloader = DataLoader(encoded_train, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Print our loss per epoch\n",
    "    print(\"Loss:\", str(epoch_loss / len(train_dataloader)))\n",
    "    pass\n",
    "\n",
    "print(\"Outside of the training loop!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42bc5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Instantiate Data Collator & Training (Mahsa)\n",
    "from transformers import AutoModelForMaskedLM\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"/tmp/\", \n",
    "                                         evaluation_strategy=\"no\",\n",
    "                                         num_train_epochs=15,\n",
    "                                         learning_rate=2e-5, \n",
    "                                         weight_decay=0.01, \n",
    "                                         per_device_train_batch_size=32,\n",
    "                                         per_device_eval_batch_size=32,\n",
    "                                         fp16=True, # Uncomment only when on GPU\n",
    "                                         push_to_hub=False)\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                         args=training_args, \n",
    "                         train_dataset=encoded_train, \n",
    "                         eval_dataset=encoded_train, \n",
    "                         data_collator=data_collator,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb27745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4905' max='4905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4905/4905 11:56, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.767600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4905, training_loss=2.1528380912135257, metrics={'train_runtime': 716.8869, 'train_samples_per_second': 218.507, 'train_steps_per_second': 6.842, 'total_flos': 3.64992688809792e+16, 'train_loss': 2.1528380912135257, 'epoch': 15.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Train Model (Mahsa)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c74a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Save Model (Kellen)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "# Replace path below with own desired path\n",
    "trainer.save_model(\"/scratch/gpfs/kc4642/Models/Trained_Numersense/bert-large-linear-FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612f9628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n"
     ]
    }
   ],
   "source": [
    "# %% Evaluate Model (Mahsa)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "def generate_json_file(input_filename,output_filename, model, tokenizer):\n",
    "    # Read all sentences from the file\n",
    "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    # Open the output JSONL file for writing\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for sentence in tqdm(sentences, desc=\"Processing sentences\", position=0, leave=True):\n",
    "            # Replace \"<mask>\" with the actual mask token used by the tokenizer\n",
    "            mask_token = tokenizer.mask_token\n",
    "            input_ids = tokenizer(sentence.replace(\"<mask>\", mask_token), return_tensors=\"pt\").input_ids\n",
    "\n",
    "            # Compute logits with no gradient calculation\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids.to(\"cuda\")).logits\n",
    "\n",
    "            # Find the position of the mask token\n",
    "            mask_token_index = (input_ids[0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "            # Apply softmax to logits at the mask token position\n",
    "            softmax_logits = torch.softmax(logits[0, mask_token_index], dim=0)\n",
    "\n",
    "            # List of candidates\n",
    "            candidates = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"no\"]\n",
    "\n",
    "            # Calculate scores for each candidate\n",
    "            results = []\n",
    "            for candidate in candidates:\n",
    "                candidate_id = tokenizer.convert_tokens_to_ids(candidate)\n",
    "                candidate_score = softmax_logits[candidate_id].item()  # Extract softmax score for each candidate\n",
    "                results.append({\"word\": candidate, \"score\": candidate_score})\n",
    "\n",
    "            # Sort results by score descending\n",
    "            results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "            # Create the result dictionary\n",
    "            result_data = {\n",
    "                \"probe\": sentence,\n",
    "                \"result_list\": results\n",
    "            }\n",
    "\n",
    "            # Write result to the JSONL file\n",
    "            json_string = json.dumps(result_data)\n",
    "            f.write(json_string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2c7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN\n",
      "Processing sentences: 100%|██████████| 1132/1132 [00:18<00:00, 62.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Extract JSON Results (Kellen)\n",
    "set_logging_and_seed(seed=42)\n",
    "\n",
    "# Define test split\n",
    "test = \"core\"\n",
    "\n",
    "if test == \"core\":\n",
    "    input_filename = \"test_initial_cleaned.txt\"\n",
    "    output_filename= \"result_core.jsonl\"\n",
    "else:\n",
    "    input_filename = \"test_dataset.txt\"\n",
    "    output_filename= \"result_all.jsonl\"\n",
    "\n",
    "# Generate our JSON results\n",
    "generate_json_file(input_filename, output_filename, model.to(\"cuda\"), tokenizer)\n",
    "print(\"Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp [~/.conda/envs/torch-nlp/]",
   "language": "python",
   "name": "conda_torch-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
